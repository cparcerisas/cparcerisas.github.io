---
---

@string{aps = {American Physical Society,}}

@article{schallRobustMethodAutomatically2022,
	title = {A {Robust} {Method} to {Automatically} {Detect} {Fin} {Whale} {Acoustic} {Presence} in {Large} and {Diverse} {Passive} {Acoustic} {Datasets}},
	volume = {10},
	url = {https://www.mdpi.com/2077-1312/10/12/1831},
	doi = {10.3390/jmse10121831},
	abstract = {The growing availability of long-term and large-scale passive acoustic recordings open the possibility of monitoring the vocal activity of elusive oceanic species, such as fin whales (Balaenoptera physalus), in order to acquire knowledge on their distribution, behavior, population structure and abundance. Fin whales produce low-frequency and high-intensity pulses, both as single vocalizations and as song sequences (only males) which can be detected over large distances. Numerous distant fin whales producing these pulses generate a so-called chorus, by spectrally and temporally overlapping single vocalizations. Both fin whale pulses and fin whale chorus provide a distinct source of information on fin whales present at different distances to the recording location. The manual review of vast amounts of passive acoustic data for the presence of single vocalizations and chorus by human experts is, however, time-consuming, often suffers from low reproducibility and in its entirety, it is practically impossible. In this publication, we present and compare robust algorithms for the automatic detection of fin whale choruses and pulses which yield good performance results (i.e., false positive rates {\textless} 3\% and true positive rates {\textgreater} 76\%) when applied to real-world passive acoustic datasets characterized by vast amounts of data, with only a small proportion of the data containing the target sounds, and diverse soundscapes from the Southern Ocean.},
	number = {12},
	journal = {Journal of Marine Science and Engineering},
	author = {Schall, Elena and Parcerisas, Clea},
	month = {Dec},
	year = {2022},
	pages = {1831}
}



@inproceedings{gaoDesignPolymerbasedPMUT2019,
	title = {Design of polymer-based {PMUT} array for multi-frequency ultrasound imaging},
	doi = {10.1109/ULTSYM.2019.8926142},
	abstract = {Multi-frequency piezoelectric micromachined transducer (PMUT) arrays have the potential to assist long-term monitoring with high resolution images at large penetration depth, paving the way for early diagnosis, follow-up and treatment. In this paper, we have demonstrated the design and characterization of multi-frequency polymer-based PMUT arrays intended for aforementioned applications. Starting from single PMUT devices, the resonance frequencies and mode shapes characterized in water agree well with the simulated counterparts. Based on these results, PMUT devices of 320 μm and 400 μm are selected to build up PMUT array. First, the maximum axial pressure of one 5×5 PMUT array has been measured in water at a frequency sweep of 1.7-20MHz. Moreover, the measured pressure map of a 16×32 PMUT array remains aligned with the acoustic simulation result. As an important step towards imaging applications, the pulse echo signal of the same PMUT array has been characterized by using a plate phantom in water.},
	booktitle = {2019 {IEEE} {International} {Ultrasonics} {Symposium} ({IUS})},
	author = {Gao, Hang and Gijsenbergh, Pieter and Halbach, Alexandre and Serrahima, Clea Parcerisas and Brondani Torri, Guilherme and Jeong, Yongbin and Billen, Margo and Cheyns, David and Haouari, Rachid and Rottenberg, Xavier and Rochus, Veronique},
	month = {Oct},
	year = {2019},
	pages = {1092--1095}
}


@article{reybaqueroComparisonTwoSoundscapes2021,
	title = {Comparison of {Two} {Soundscapes}: {An} {Opportunity} to {Assess} the {Dominance} of {Biophony} {Versus} {Anthropophony}},
	url = {https://tos.org/oceanography/article/comparison-of-two-soundscapes-an-opportunity-to-assess-the-dominance-of-biophony-versus-anthropophony},
	doi = {10.5670/oceanog.2021.supplement.02-24},
	journal = {Oceanography},
	author = {Rey Baquero, Maria Paula and Parcerisas, Clea and Seger, Kerri and Perazio, Christina and Botero Acosta, Natalia and Mesa, Felipe and Acosta, Andrea Luna and Botteldooren, Dick and Debusschere, Elisabeth},
	month = {Dec},
	year = {2021},
	pages = {62--65}
}




@article{parcerisasCategorizingShallowMarine2023,
	title = {Categorizing {Shallow} {Marine} {Soundscapes} {Using} {Explained} {Clusters}},
	volume = {11},
	url = {https://www.mdpi.com/2077-1312/11/3/550},
	doi = {10.3390/jmse11030550},
	abstract = {Natural marine soundscapes are being threatened by increasing anthropic noise, particularly in shallow coastal waters. To preserve and monitor these soundscapes, understanding them is essential. Here, we propose a new method for semi-supervised categorization of shallow marine soundscapes, with further interpretation of these categories according to concurrent environmental conditions. The proposed methodology uses a nonlinear mapping of short-term spectrograms to a two-dimensional space, followed by a density-based clustering algorithm to identify similar sound environments. A random forest classifier, based on additional environmental data, is used to predict their occurrence. Finally, explainable machine learning tools provide insight into the ecological explanation of the clusters. This methodology was tested in the Belgian part of the North Sea, and resulted in clearly identifiable categories of soundscapes that could be explained by spatial and temporal environmental parameters, such as distance to the shore, bathymetry, tide or season. Classifying soundscapes facilitates their identification, which can be useful for policy making or conservation programs. Soundscape categorization, as proposed in this work, could be used to monitor acoustic trends and patterns in space and time that might provide useful indicators of biodiversity and ecosystem functionality change.},
	number = {3},
	journal = {Journal of Marine Science and Engineering},
	author = {Parcerisas, Clea and Roca, Irene T. and Botteldooren, Dick and Devos, Paul and Debusschere, Elisabeth},
	month = {Mar},
	year = {2023},
	pages = {550}
}

@article{rubbensMachineLearningMarine2023,
	title = {Machine learning in marine ecology: an overview of techniques and applications},
	url = {https://doi.org/10.1093/icesjms/fsad100},
	doi = {10.1093/icesjms/fsad100},
	abstract = {Machine learning covers a large set of algorithms that can be trained to identify patterns in data. Thanks to the increase in the amount of data and computing power available, it has become pervasive across scientific disciplines. We first highlight why machine learning is needed in marine ecology. Then we provide a quick primer on machine learning techniques and vocabulary. We built a database of ∼1000 publications that implement such techniques to analyse marine ecology data. For various data types (images, optical spectra, acoustics, omics, geolocations, biogeochemical profiles, and satellite imagery), we present a historical perspective on applications that proved influential, can serve as templates for new work, or represent the diversity of approaches. Then, we illustrate how machine learning can be used to better understand ecological systems, by combining various sources of marine data. Through this coverage of the literature, we demonstrate an increase in the proportion of marine ecology studies that use machine learning, the pervasiveness of images as a data source, the dominance of machine learning for classification-type problems, and a shift towards deep learning for all data types. This overview is meant to guide researchers who wish to apply machine learning methods to their marine datasets.},
	journal = {ICES Journal of Marine Science},
	author = {Rubbens, Peter and Brodie, Stephanie and Cordier, Tristan and Destro Barcellos, Diogo and Devos, Paul and Fernandes-Salvador, Jose A and Fincham, Jennifer I and Gomes, Alessandra and Handegard, Nils Olav and Howell, Kerry and Jamet, Cédric and Kartveit, Kyrre Heldal and Moustahfid, Hassan and Parcerisas, Clea and Politikos, Dimitris and Sauzède, Raphaëlle and Sokolova, Maria and Uusitalo, Laura and Van den Bulcke, Laure and van Helmond, Aloysius T M and Watson, Jordan T and Welch, Heather and Beltran-Perez, Oscar and Chaffron, Samuel and Greenberg, David S and Kühn, Bernhard and Kiko, Rainer and Lo, Madiop and Lopes, Rubens M and Möller, Klas Ove and Michaels, William and Pala, Ahmet and Romagnan, Jean-Baptiste and Schuchert, Pia and Seydi, Vahid and Villasante, Sebastian and Malde, Ketil and Irisson, Jean-Olivier},
	month = {Aug},
	year = {2023},
}

@inproceedings{parcerisasStudyingSoundscapeShallow2023,
	title = {Studying the {Soundscape} of {Shallow} and {Heavy} {Used} {Marine} {Areas}: {Belgian} {Part} of the {North} {Sea}},
	url = {https://link.springer.com/10.1007/978-3-031-10417-6_122-1},
	abstract = {The impact of anthropogenic sound on marine fauna is a growing concern, particularly in shallow, coastal, and heavily exploited marine areas such as the Belgian Part of the North Sea (BPNS). Understanding the ecosystem and its limits in these areas is necessary to protect these areas and ensure their sustainable use. To quantify this impact, characterizing and analyzing the soundscape is crucial. However, analyzing soundscapes in shallow and heavily exploited marine areas poses several challenges and particularities. Bio-fouling, ﬂow-noise, unknown sound sources, and masking compromise propagation. This chapter provides an overview of the soundscape in the BPNS and the inherent challenges to measure and analyze it. Some of the challenges are exempliﬁed using data collected in the framework of the LifeWatch Broadband Acoustic Network.},
	booktitle = {The {Effects} of {Noise} on {Aquatic} {Life}},
	publisher = {Springer International Publishing},
	author = {Parcerisas, Clea and Botteldooren, Dick and Devos, Paul and Hamard, Quentin and Debusschere, Elisabeth},
	editor = {Popper, Arthur N. and Sisneros, Joseph and Hawkins, Anthony D. and Thomsen, Frank},
	year = {2023},
	doi = {10.1007/978-3-031-10417-6_122-1},
	pages = {1--27}
}



@inproceedings{parcerisasCLUSTERINGCATEGORIZINGMAPPING2023,
	title = {{CLUSTERING}, {CATEGORIZING}, {AND} {MAPPING} {OF} {SHALLOW} {COASTAL} {WATER} {SOUNDSCAPES}},
	doi = {DOI:10.61782/fa.2023.1070},
	abstract = {For many of its inhabitants, the underwater soundscape is a rich source of information that may be crucial for their survival. Moreover, in shallow coastal waters where visibility is poor, the importance of sound is emphasized. Yet coastal waters are also rich in anthropogenic sounds which may disturb the ecosystem. Passive Acoustics Monitoring (PAM) is a flexible, non-invasive, and cost-effective solution to acquire information at habitat or community level. Studying the acoustic scene of a habitat in a global, holistic way is known as soundscape analysis. However, there are currently no standardized methods to characterize and understand marine soundscapes in an automated way. Here we propose a methodology for clustering underwater soundscapes and linking the obtained categories to environmental parameters in space and time. This is done using explainable artificial intelligence. The methodology is applied to a PAM dataset collected in the Belgian Part of the North Sea. The obtained categories focus on background sound, which includes all combinations of sounds that occur under certain conditions at specific places. With this information, the marine acoustic scene and its change over space and time can be mapped for the whole area of interest.},
	booktitle = {Proceedings of the 10th {Convention} of the {European} {Acoustics} {Association} {Forum} {Acusticum} 2023},
	author = {Parcerisas, Clea and Botteldooren, Dick and Devos, Paul and Debusschere, Elisabeth},
	month = {Sep},
	year = {2023},
}

@article{parcerisasMachineLearningEfficient2024,
	title = {Machine learning for efficient segregation and labeling of potential biological sounds in long-term underwater recordings},
	volume = {5},
	url = {https://www.frontiersin.org/articles/10.3389/frsen.2024.1390687},
	doi = {10.3389/frsen.2024.1390687},
	abstract = {Studying marine soundscapes by detecting known sound events and quantifying their spatio-temporal patterns can provide ecologically relevant information. However, the exploration of underwater sound data to find and identify possible sound events of interest can be highly time-intensive for human analysts. To speed up this process, we propose a novel methodology that first detects all the potentially relevant acoustic events and then clusters them in an unsupervised way prior to manual revision. We demonstrate its applicability on a short deployment. To detect acoustic events, a deep learning object detection algorithm from computer vision (YOLOv8) is re-trained to detect any (short) acoustic event. This is done by converting the audio to spectrograms using sliding windows longer than the expected sound events of interest. The model detects any event present on that window and provides their time and frequency limits. With this approach, multiple events happening simultaneously can be detected. To further explore the possibilities to limit the human input needed to create the annotations to train the model, we propose an active learning approach to select the most informative audio files in an iterative manner for subsequent manual annotation. The obtained detection models are trained and tested on a dataset from the Belgian Part of the North Sea, and then further evaluated for robustness on a freshwater dataset from major European rivers. The proposed active learning approach outperforms the random selection of files, both in the marine and the freshwater datasets. Once the events are detected, they are converted to an embedded feature space using the BioLingual model, which is trained to classify different (biological) sounds. The obtained representations are then clustered in an unsupervised way, obtaining different sound classes. These classes are then manually revised. This method can be applied to unseen data as a tool to help bioacousticians identify recurrent sounds and save time when studying their spatio-temporal patterns. This reduces the time researchers need to go through long acoustic recordings and allows to conduct a more targeted analysis. It also provides a framework to monitor soundscapes regardless of whether the sound sources are known or not.},
	journal = {Frontiers in Remote Sensing},
	author = {Parcerisas, Clea and Schall, Elena and te Velde, Kees and Botteldooren, Dick and Devos, Paul and Debusschere, Elisabeth},
	month = {Apr},
	year = {2024},
	selected = true
}

@article{schallDeepLearningMarine2024,
	title = {Deep learning in marine bioacoustics: a benchmark for baleen whale detection},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rse2.392},
	doi = {10.1002/rse2.392},
	abstract = {Passive acoustic monitoring (PAM) is commonly used to obtain year-round continuous data on marine soundscapes harboring valuable information on species distributions or ecosystem dynamics. This continuously increasing amount of data requires highly efficient automated analysis techniques in order to exploit the full potential of the available data. Here, we propose a benchmark, which consists of a public dataset, a well-defined task and evaluation procedure to develop and test automated analysis techniques. This benchmark focuses on the special case of detecting animal vocalizations in a real-world dataset from the marine realm. We believe that such a benchmark is necessary to monitor the progress in the development of new detection algorithms in the field of marine bioacoustics. We ultimately use the proposed benchmark to test three detection approaches, namely ANIMAL-SPOT, Koogu and a simple custom sequential convolutional neural network (CNN), and report performances. We report the performance of the three detection approaches in a blocked cross-validation fashion with 11 site-year blocks for a multi-species detection scenario in a large marine passive acoustic dataset. Performance was measured with three simple metrics (i.e., true classification rate, noise misclassification rate and call misclassification rate) and one combined fitness metric, which allocates more weight to the minimization of false positives created by noise. Overall, ANIMAL-SPOT performed the best with an average fitness metric of 0.6, followed by the custom CNN with an average fitness metric of 0.57 and finally Koogu with an average fitness metric of 0.42. The presented benchmark is an important step to advance in the automatic processing of the continuously growing amount of PAM data that are collected throughout the world's oceans. To ultimately achieve usability of developed algorithms, the focus of future work should be laid on the reduction of the false positives created by noise.},
	journal = {Remote Sensing in Ecology and Conservation},
	author = {Schall, Elena and Kaya, Idil Ilgaz and Debusschere, Elisabeth and Devos, Paul and Parcerisas, Clea},
	month = {Apr},
	year = {2024},
	selected = true
}

@article{schmidlinComparisonEffectsReef2024,
	title = {Comparison of the effects of reef and anthropogenic soundscapes on oyster larvae settlement},
	volume = {14},
	url = {https://doi.org/10.1038/s41598-024-63322-2},
	doi = {10.1038/s41598-024-63322-2},
	abstract = {Settlement is a critical period in the life cycle of marine invertebrates with a planktonic larval stage. For reef-building invertebrates such as oysters and corals, settlement rates are predictive for long-term reef survival. Increasing evidence suggests that marine invertebrates use information from ocean soundscapes to inform settlement decisions. Sessile marine invertebrates with a planktonic stage are particularly reliant on environmental cues to direct them to ideal habitats. As gregarious settlers, oysters prefer to settle amongst members of the same species. It has been hypothesized that oyster larvae from species Crassostrea virginica and Ostrea angasi use distinct conspecific oyster reef sounds to navigate to ideal habitats. In controlled laboratory experiments we exposed Pacific Oyster Magallana gigas larvae to anthropogenic sounds from conspecific oyster reefs, vessels, combined reef-vessel sounds as well as off-reef and no speaker controls. Our findings show that sounds recorded at conspecific reefs induced higher percentages of settlement by about 1.44 and 1.64 times compared to off-reef and no speaker controls, respectively. In contrast, the settlement increase compared to the no speaker control was non-significant for vessel sounds (1.21 fold), combined reef-vessel sounds (1.30 fold), and off-reef sounds (1.18 fold). This study serves as a foundational stepping stone for exploring larval sound feature preferences within this species.},
	number = {1},
	journal = {Scientific Reports},
	author = {Schmidlin, Sarah and Parcerisas, Clea and Hubert, Jeroen and Watson, Maryann S. and Mees, Jan and Botteldooren, Dick and Devos, Paul and Debusschere, Elisabeth and Hablützel, Pascal I.},
	month = {May},
	year = {2024},
	pages = {12580},
	selected = true
}

@article{calongeRevisedClustersAnnotated2024,
	title = {Revised clusters of annotated unknown sounds in the {Belgian} part of the {North} sea},
	volume = {5},
	url = {https://www.frontiersin.org/articles/10.3389/frsen.2024.1384562},
	doi = {10.3389/frsen.2024.1384562},
	abstract = {Acoustic signals, especially those of biological source, remain unexplored in the Belgian part of the North Sea (BPNS). The BPNS, although dominated by anthrophony (sounds from human activities), is expected to be acoustically diverse given the presence of biodiverse sandbanks, gravel beds and artificial hard structures. Under the framework of the LifeWatch Broadband Acoustic Network, sound data have been collected since the spring of 2020. These recordings, encompassing both biophony, geophony and anthrophony, have been listened to and annotated for unknown, acoustically salient sounds. To obtain the acoustic features of these annotations, we used two existing automatic feature extractions: the Animal Vocalization Encoder based on Self-Supervision (AVES) and a convolutional autoencoder network (CAE) retrained on the data from this study. An unsupervised density-based clustering algorithm (HDBSCAN) was applied to predict clusters. We coded a grid search function to reduce the dimensionality of the feature sets and to adjusttune the hyperparameters of HDBSCAN. We searched the hyperparameter space for the most optimized combination of parameter values based on two selected clustering evaluation measures: the homogeneity and the density-based clustering validation (DBCV) scores. Although both feature sets produced meaningful clusters, AVES feature sets resulted in more solid, homogeneous clusters with relatively lower intra-cluster distances, appearing to be more advantageous for the purpose and dataset of this study. The 26 final clusters we obtained were revised by a bioacoustics expert. , of which wWe were able to name and describe 10 unique sounds, but only clusters named as 'Jackhammer' and 'Tick' can be interpreted as biological with certainty. Although unsupervised clustering is conventional in ecological research, we highlight its practical use in revising clusters of annotated unknown sounds. The revised clusters we detailed in this study already define a few groups of distinct and recurring sounds that could serve as a preliminary component of a valid annotated training dataset potentially feeding supervised machine learning and classifier models.},
	journal = {Frontiers in Remote Sensing},
	author = {Calonge, Arienne and Parcerisas, Clea and Schall, Elena and Debusschere, Elisabeth},
	month = {Jun},
	year = {2024}
}
