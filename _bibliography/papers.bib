@report{boebel21PhysicalOceanography,
  title = {2.1. {{Physical Oceanography}}, in: {{Hoppema}}, {{M}}. {{The Expedition PS129}} of the {{Research Vessel POLARSTERN}} to the {{Weddell Sea}} in 2022. {{Berichte}} Zur {{Polar-}} Und {{Meeresforschung}} = {{Reports}} on {{Polar}} and {{Marine Research}}},
  author = {Boebel, Olaf and Allerholt, Jakob and Engicht, Carina and Hoppema, Mario and Llanillo, Pedro and Parcerisas, Clea and Pinner, Ole and Roca, Irene T. and Spiesecke, Stefanie and Tippenhauer, Sandra},
  pages = {18--52},
  institution = {Alfred Wegener Institute},
  url = {https://epic.awi.de/id/eprint/57797/}
}

@online{calongeMultipurposeSeabedMoorings2024,
  title = {Multipurpose Seabed Moorings: Enabling Sustained Long-Term, Continuous Observations in Shallow Waters (in Prep.)},
  author = {Calonge, Arienne and Develter, Roeland and Parcerisas, Clea and Muñiz, Carlota and Reubens, Jan and Boone, Wieter and Deneudt, Klaas and Debusschere, Elisabeth},
  date = {2024},
  pubstate = {prepublished}
}

@article{calongeRevisedClustersAnnotated2024,
  title = {Revised Clusters of Annotated Unknown Sounds in the {{Belgian}} Part of the {{North}} Sea},
  author = {Calonge, Arienne and Parcerisas, Clea and Schall, Elena and Debusschere, Elisabeth},
  date = {2024-06-04},
  journaltitle = {Frontiers in Remote Sensing},
  shortjournal = {Front. Remote Sens.},
  volume = {5},
  publisher = {Frontiers},
  issn = {2673-6187},
  doi = {10.3389/frsen.2024.1384562},
  url = {https://www.frontiersin.org/articles/10.3389/frsen.2024.1384562},
  urldate = {2024-06-05},
  abstract = {Acoustic signals, especially those of biological source, remain unexplored in the Belgian part of the North Sea (BPNS). The BPNS, although dominated by anthrophony (sounds from human activities), is expected to be acoustically diverse given the presence of biodiverse sandbanks, gravel beds and artificial hard structures. Under the framework of the LifeWatch Broadband Acoustic Network, sound data have been collected since the spring of 2020. These recordings, encompassing both biophony, geophony and anthrophony, have been listened to and annotated for unknown, acoustically salient sounds. To obtain the acoustic features of these annotations, we used two existing automatic feature extractions: the Animal Vocalization Encoder based on Self-Supervision (AVES) and a convolutional autoencoder network (CAE) retrained on the data from this study. An unsupervised density-based clustering algorithm (HDBSCAN) was applied to predict clusters. We coded a grid search function to reduce the dimensionality of the feature sets and to adjusttune the hyperparameters of HDBSCAN. We searched the hyperparameter space for the most optimized combination of parameter values based on two selected clustering evaluation measures: the homogeneity and the density-based clustering validation (DBCV) scores. Although both feature sets produced meaningful clusters, AVES feature sets resulted in more solid, homogeneous clusters with relatively lower intra-cluster distances, appearing to be more advantageous for the purpose and dataset of this study. The 26 final clusters we obtained were revised by a bioacoustics expert. , of which wWe were able to name and describe 10 unique sounds, but only clusters named as 'Jackhammer' and 'Tick' can be interpreted as biological with certainty. Although unsupervised clustering is conventional in ecological research, we highlight its practical use in revising clusters of annotated unknown sounds. The revised clusters we detailed in this study already define a few groups of distinct and recurring sounds that could serve as a preliminary component of a valid annotated training dataset potentially feeding supervised machine learning and classifier models.},
  langid = {english},
  keywords = {annotation,Autoencoder,Aves,Bioacoustic,Grid search,Training dataset,unknown soundscape,unsupervised},
  file = {C:\Users\cleap\Zotero\storage\EGF4V5WH\Calonge et al. - 2024 - Revised clusters of annotated unknown sounds in th.pdf}
}

@inproceedings{gaoDesignPolymerbasedPMUT2019,
  title = {Design of Polymer-Based {{PMUT}} Array for Multi-Frequency Ultrasound Imaging},
  booktitle = {2019 {{IEEE International Ultrasonics Symposium}} ({{IUS}})},
  author = {Gao, Hang and Gijsenbergh, Pieter and Halbach, Alexandre and Serrahima, Clea Parcerisas and Brondani Torri, Guilherme and Jeong, Yongbin and Billen, Margo and Cheyns, David and Haouari, Rachid and Rottenberg, Xavier and Rochus, Veronique},
  date = {2019-10},
  pages = {1092--1095},
  issn = {1948-5727},
  doi = {10.1109/ULTSYM.2019.8926142},
  abstract = {Multi-frequency piezoelectric micromachined transducer (PMUT) arrays have the potential to assist long-term monitoring with high resolution images at large penetration depth, paving the way for early diagnosis, follow-up and treatment. In this paper, we have demonstrated the design and characterization of multi-frequency polymer-based PMUT arrays intended for aforementioned applications. Starting from single PMUT devices, the resonance frequencies and mode shapes characterized in water agree well with the simulated counterparts. Based on these results, PMUT devices of 320 μm and 400 μm are selected to build up PMUT array. First, the maximum axial pressure of one 5×5 PMUT array has been measured in water at a frequency sweep of 1.7-20MHz. Moreover, the measured pressure map of a 16×32 PMUT array remains aligned with the acoustic simulation result. As an important step towards imaging applications, the pulse echo signal of the same PMUT array has been characterized by using a plate phantom in water.},
  eventtitle = {2019 {{IEEE International Ultrasonics Symposium}} ({{IUS}})},
  keywords = {Frequency measurement,Imaging,mode shapes,multi-frequency imaging,PMUT,polymer-based,pressure,pulse-echo.,resonance frequency,Resonant frequency,Shape,Transducers,Ultrasonic imaging,Ultrasonic variables measurement},
  file = {C:\Users\cleap\Zotero\storage\JEM7BTEC\8926142.html}
}

@dataset{parcerisasAcousticSalientEvent2024,
  title = {Acoustic Salient Event Annotations},
  author = {Parcerisas, Clea and Schall, Elena and Aubach and family=Velde, given=Kees, prefix=te, useprefix=true and Slabbekoorn, Hans and Debusschere, Elisabeth},
  date = {2024},
  doi = {10.14284/667},
  url = {https://www.vliz.be/en/imis?module=dataset&dasid=8543}
}

@dataset{parcerisasAnnotatedUnknownUnderwater2024,
  title = {Annotated Unknown Underwater Sounds in the {{Belgian}} Part of the {{North Sea}}},
  author = {Parcerisas, Clea and Schall, Elena and Calonge, Arienne and Debusschere, Elisabeth},
  date = {2024},
  doi = {10.14284/659},
  url = {https://www.vliz.be/en/imis?module=dataset&dasid=8520}
}

@dataset{parcerisasBroadbandAcousticNetwork2021,
  title = {Broadband {{Acoustic Network}} Dataset},
  author = {Parcerisas, Clea and Botteldooren, Dick and Devos, Paul and Debusschere, Elisabeth},
  date = {2021},
  publisher = {Flanders Marine Institute (VLIZ)},
  url = {https://www.vliz.be/en/imis?module=dataset&dasid=7879},
  urldate = {2022-12-19},
  abstract = {Underwater Acoustic Network recording continuously from 10 Hz to 50 kHz, covering most of geophonic sounds, anthropogenic noise and biophonic events in the Belgian Part of the North Sea Flanders Marine Institute - Platform for marine research},
  langid = {english},
  annotation = {https://www.lifewatch.be/en/broadband-acoustic-network},
  file = {C:\Users\cleap\Zotero\storage\XMIQBJ7X\imis.html}
}

@article{parcerisasCategorizingShallowMarine2023,
  title = {Categorizing {{Shallow Marine Soundscapes Using Explained Clusters}}},
  author = {Parcerisas, Clea and Roca, Irene T. and Botteldooren, Dick and Devos, Paul and Debusschere, Elisabeth},
  date = {2023-03},
  journaltitle = {Journal of Marine Science and Engineering},
  volume = {11},
  number = {3},
  pages = {550},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2077-1312},
  doi = {10.3390/jmse11030550},
  url = {https://www.mdpi.com/2077-1312/11/3/550},
  urldate = {2023-03-08},
  abstract = {Natural marine soundscapes are being threatened by increasing anthropic noise, particularly in shallow coastal waters. To preserve and monitor these soundscapes, understanding them is essential. Here, we propose a new method for semi-supervised categorization of shallow marine soundscapes, with further interpretation of these categories according to concurrent environmental conditions. The proposed methodology uses a nonlinear mapping of short-term spectrograms to a two-dimensional space, followed by a density-based clustering algorithm to identify similar sound environments. A random forest classifier, based on additional environmental data, is used to predict their occurrence. Finally, explainable machine learning tools provide insight into the ecological explanation of the clusters. This methodology was tested in the Belgian part of the North Sea, and resulted in clearly identifiable categories of soundscapes that could be explained by spatial and temporal environmental parameters, such as distance to the shore, bathymetry, tide or season. Classifying soundscapes facilitates their identification, which can be useful for policy making or conservation programs. Soundscape categorization, as proposed in this work, could be used to monitor acoustic trends and patterns in space and time that might provide useful indicators of biodiversity and ecosystem functionality change.},
  issue = {3},
  langid = {english},
  keywords = {eco-acoustics,interpretable machine learning,marine soundscape,Random Forest,shallow water,SHAP,UMAP,XAI},
  file = {C:\Users\cleap\Zotero\storage\LERZN3SU\Parcerisas et al. - 2023 - Categorizing Shallow Marine Soundscapes Using Expl.pdf}
}

@inproceedings{parcerisasCLUSTERINGCATEGORIZINGMAPPING2023,
  title = {{{CLUSTERING}}, {{CATEGORIZING}}, {{AND MAPPING OF SHALLOW COASTAL WATER SOUNDSCAPES}}},
  booktitle = {Proceedings of the 10th {{Convention}} of the {{European Acoustics Association Forum Acusticum}} 2023},
  author = {Parcerisas, Clea and Botteldooren, Dick and Devos, Paul and Debusschere, Elisabeth},
  date = {2023-09},
  location = {Torino, Italy},
  doi = {DOI:10.61782/fa.2023.1070},
  abstract = {For many of its inhabitants, the underwater soundscape is a rich source of information that may be crucial for their survival. Moreover, in shallow coastal waters where visibility is poor, the importance of sound is emphasized. Yet coastal waters are also rich in anthropogenic sounds which may disturb the ecosystem. Passive Acoustics Monitoring (PAM) is a flexible, non-invasive, and cost-effective solution to acquire information at habitat or community level. Studying the acoustic scene of a habitat in a global, holistic way is known as soundscape analysis. However, there are currently no standardized methods to characterize and understand marine soundscapes in an automated way. Here we propose a methodology for clustering underwater soundscapes and linking the obtained categories to environmental parameters in space and time. This is done using explainable artificial intelligence. The methodology is applied to a PAM dataset collected in the Belgian Part of the North Sea. The obtained categories focus on background sound, which includes all combinations of sounds that occur under certain conditions at specific places. With this information, the marine acoustic scene and its change over space and time can be mapped for the whole area of interest.},
  eventtitle = {Forum {{Acusticum}}},
  langid = {english},
  file = {C:\Users\cleap\Zotero\storage\CDFQXF6X\Parcerisas et al. - 2023 - CLUSTERING, CATEGORIZING, AND MAPPING OF SHALLOW C.pdf}
}

@unpublished{parcerisasDetectingClusteringUnknown2024,
  title = {Detecting and Clustering Unknown Sound Events Using Transfer Learning for Marine Soundscape Analysis},
  author = {Parcerisas, Clea and Schall, Elena and Botteldooren, Dick and Devos, Paul and Debusschere, Elisabeth},
  date = {2024-06},
  url = {https://icua2024.org/},
  eventtitle = {{{ICUA}}},
  venue = {Bath}
}

@software{parcerisasLifewatchBpnsdataFirst2021,
  title = {Lifewatch/Bpnsdata: {{First}} Release of Bpnsdata},
  shorttitle = {Lifewatch/Bpnsdata},
  author = {Parcerisas, Clea},
  date = {2021-11-03},
  doi = {10.5281/zenodo.5642499},
  url = {https://zenodo.org/record/5642499},
  urldate = {2021-12-02},
  abstract = {First release of bpnsdata, where several environmental data can be added to a GeoDataFrame (geopandas)},
  organization = {Zenodo},
  file = {C:\Users\cleap\Zotero\storage\B8BW35Z8\5642499.html}
}

@software{parcerisasLifewatchPyhydrophoneProcess2023,
  title = {Lifewatch/Pyhydrophone: Process Hydrophone Data in Standarized and Easy Way},
  shorttitle = {Lifewatch/Pyhydrophone},
  author = {Parcerisas, Clea},
  date = {2023-01-31},
  doi = {10.5281/zenodo.7588428},
  url = {https://zenodo.org/record/7588428},
  urldate = {2023-02-03},
  organization = {Zenodo},
  file = {C:\Users\cleap\Zotero\storage\LCLQVSRQ\7588428.html}
}

@software{parcerisasLifewatchPypamPypam2022,
  title = {Lifewatch/Pypam: Pypam, a Package to Process Bioacoustic Data},
  shorttitle = {Lifewatch/Pypam},
  author = {Parcerisas, Clea},
  date = {2022-02-11},
  doi = {10.5281/zenodo.6044593},
  url = {https://zenodo.org/record/6044593},
  urldate = {2022-02-11},
  abstract = {Dataset to process multiple deployments included. Bugs solved.},
  organization = {Zenodo},
  file = {C:\Users\cleap\Zotero\storage\35EYWJJD\6044593.html}
}

@software{parcerisasLifewatchPyporccPython2021,
  title = {Lifewatch/Pyporcc: {{A}} Python Package to Detect and Classify Porpoise Clicks},
  shorttitle = {Lifewatch/Pyporcc},
  author = {Parcerisas, Clea},
  date = {2021-08-11},
  doi = {10.5281/zenodo.5179943},
  url = {https://zenodo.org/record/5179943},
  urldate = {2021-08-11},
  abstract = {Python tool to detect and classify harbor porpoise's clicks},
  organization = {Zenodo},
  file = {C:\Users\cleap\Zotero\storage\CJA6E986\5179943.html}
}

@article{parcerisasMachineLearningEfficient2024,
  title = {Machine Learning for Efficient Segregation and Labeling of Potential Biological Sounds in Long-Term Underwater Recordings},
  author = {Parcerisas, Clea and Schall, Elena and family=Velde, given=Kees, prefix=te, useprefix=true and Botteldooren, Dick and Devos, Paul and Debusschere, Elisabeth},
  date = {2024-04-25},
  journaltitle = {Frontiers in Remote Sensing},
  shortjournal = {Front. Remote Sens.},
  volume = {5},
  publisher = {Frontiers},
  issn = {2673-6187},
  doi = {10.3389/frsen.2024.1390687},
  url = {https://www.frontiersin.org/articles/10.3389/frsen.2024.1390687},
  urldate = {2024-05-16},
  abstract = {Studying marine soundscapes by detecting known sound events and quantifying their spatio-temporal patterns can provide ecologically relevant information. However, the exploration of underwater sound data to find and identify possible sound events of interest can be highly time-intensive for human analysts. To speed up this process, we propose a novel methodology that first detects all the potentially relevant acoustic events and then clusters them in an unsupervised way prior to manual revision. We demonstrate its applicability on a short deployment. To detect acoustic events, a deep learning object detection algorithm from computer vision (YOLOv8) is re-trained to detect any (short) acoustic event. This is done by converting the audio to spectrograms using sliding windows longer than the expected sound events of interest. The model detects any event present on that window and provides their time and frequency limits. With this approach, multiple events happening simultaneously can be detected. To further explore the possibilities to limit the human input needed to create the annotations to train the model, we propose an active learning approach to select the most informative audio files in an iterative manner for subsequent manual annotation. The obtained detection models are trained and tested on a dataset from the Belgian Part of the North Sea, and then further evaluated for robustness on a freshwater dataset from major European rivers. The proposed active learning approach outperforms the random selection of files, both in the marine and the freshwater datasets. Once the events are detected, they are converted to an embedded feature space using the BioLingual model, which is trained to classify different (biological) sounds. The obtained representations are then clustered in an unsupervised way, obtaining different sound classes. These classes are then manually revised. This method can be applied to unseen data as a tool to help bioacousticians identify recurrent sounds and save time when studying their spatio-temporal patterns. This reduces the time researchers need to go through long acoustic recordings and allows to conduct a more targeted analysis. It also provides a framework to monitor soundscapes regardless of whether the sound sources are known or not.},
  langid = {english},
  keywords = {clustering,object detection,soundscape,Transfer Learning,underwater sound,Unknown sounds},
  file = {C:\Users\cleap\Zotero\storage\BAX85R8D\Parcerisas et al. - 2024 - Machine learning for efficient segregation and lab.pdf}
}

@unpublished{parcerisasNewDeepLearning2024,
  title = {A New Deep Learning Model Evaluated on the {{Antarctic}} Benchmark for Baleen Whale Calls},
  author = {Parcerisas, Clea and Kaya, Idil Ilgaz and Devos, Paul and Debusschere, Elisabeth and Schall, Elena},
  date = {2024-06},
  url = {https://www.dclde2024.com},
  eventtitle = {{{DCLDE}}},
  venue = {Rotterdam}
}

@inproceedings{parcerisasStudyingSoundscapeShallow2023,
  title = {Studying the {{Soundscape}} of {{Shallow}} and {{Heavy Used Marine Areas}}: {{Belgian Part}} of the {{North Sea}}},
  shorttitle = {Studying the {{Soundscape}} of {{Shallow}} and {{Heavy Used Marine Areas}}},
  booktitle = {The {{Effects}} of {{Noise}} on {{Aquatic Life}}},
  author = {Parcerisas, Clea and Botteldooren, Dick and Devos, Paul and Hamard, Quentin and Debusschere, Elisabeth},
  editor = {Popper, Arthur N. and Sisneros, Joseph and Hawkins, Anthony D. and Thomsen, Frank},
  date = {2023},
  pages = {1--27},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-10417-6_122-1},
  url = {https://link.springer.com/10.1007/978-3-031-10417-6_122-1},
  urldate = {2023-09-10},
  abstract = {The impact of anthropogenic sound on marine fauna is a growing concern, particularly in shallow, coastal, and heavily exploited marine areas such as the Belgian Part of the North Sea (BPNS). Understanding the ecosystem and its limits in these areas is necessary to protect these areas and ensure their sustainable use. To quantify this impact, characterizing and analyzing the soundscape is crucial. However, analyzing soundscapes in shallow and heavily exploited marine areas poses several challenges and particularities. Bio-fouling, flow-noise, unknown sound sources, and masking compromise propagation. This chapter provides an overview of the soundscape in the BPNS and the inherent challenges to measure and analyze it. Some of the challenges are exemplified using data collected in the framework of the LifeWatch Broadband Acoustic Network.},
  isbn = {978-3-031-10417-6},
  langid = {english},
  file = {C:\Users\cleap\Zotero\storage\LS93LDXX\Parcerisas et al. - 2023 - Studying the Soundscape of Shallow and Heavy Used .pdf}
}

@unpublished{parcerisasUsingCNNClassifiers2023,
  title = {Using {{CNN}} Classifiers as Underwater Sound Source Detectors: Learning about Noise.},
  author = {Parcerisas, Clea and Kaya, Idil Ilgaz and Botteldooren, Dick and Devos, Paul and Debusschere, Elisabeth and Schall, Elena},
  date = {2023-05},
  url = {https://2023.oceanoise.com/},
  eventtitle = {{{OCEANOISE}}},
  venue = {Vilanova i la Geltru, Spain}
}

@article{reybaqueroComparisonTwoSoundscapes2021,
  title = {Comparison of {{Two Soundscapes}}: {{An Opportunity}} to {{Assess}} the {{Dominance}} of {{Biophony Versus Anthropophony}}},
  shorttitle = {Comparison of {{Two Soundscapes}}},
  author = {Rey Baquero, Maria Paula and Parcerisas, Clea and Seger, Kerri and Perazio, Christina and Botero Acosta, Natalia and Mesa, Felipe and Acosta, Andrea Luna and Botteldooren, Dick and Debusschere, Elisabeth},
  date = {2021-12-01},
  journaltitle = {Oceanography},
  shortjournal = {Oceanog},
  pages = {62--65},
  issn = {10428275},
  doi = {10.5670/oceanog.2021.supplement.02-24},
  url = {https://tos.org/oceanography/article/comparison-of-two-soundscapes-an-opportunity-to-assess-the-dominance-of-biophony-versus-anthropophony},
  urldate = {2023-02-03},
  langid = {english},
  file = {C:\Users\cleap\Zotero\storage\QJGK3LMA\Pontificia Universidad Javeriana et al. - 2021 - Comparison of Two Soundscapes An Opportunity to A.pdf}
}

@article{rubbensMachineLearningMarine2023,
  title = {Machine Learning in Marine Ecology: An Overview of Techniques and Applications},
  shorttitle = {Machine Learning in Marine Ecology},
  author = {Rubbens, Peter and Brodie, Stephanie and Cordier, Tristan and Destro~Barcellos, Diogo and Devos, Paul and Fernandes-Salvador, Jose A and Fincham, Jennifer I and Gomes, Alessandra and Handegard, Nils Olav and Howell, Kerry and Jamet, Cédric and Kartveit, Kyrre Heldal and Moustahfid, Hassan and Parcerisas, Clea and Politikos, Dimitris and Sauzède, Raphaëlle and Sokolova, Maria and Uusitalo, Laura and Van~den~Bulcke, Laure and {van~Helmond}, Aloysius T M and Watson, Jordan T and Welch, Heather and Beltran-Perez, Oscar and Chaffron, Samuel and Greenberg, David S and Kühn, Bernhard and Kiko, Rainer and Lo, Madiop and Lopes, Rubens M and Möller, Klas Ove and Michaels, William and Pala, Ahmet and Romagnan, Jean-Baptiste and Schuchert, Pia and Seydi, Vahid and Villasante, Sebastian and Malde, Ketil and Irisson, Jean-Olivier},
  date = {2023-08-03},
  journaltitle = {ICES Journal of Marine Science},
  shortjournal = {ICES Journal of Marine Science},
  pages = {fsad100},
  issn = {1054-3139},
  doi = {10.1093/icesjms/fsad100},
  url = {https://doi.org/10.1093/icesjms/fsad100},
  urldate = {2023-08-04},
  abstract = {Machine learning covers a large set of algorithms that can be trained to identify patterns in data. Thanks to the increase in the amount of data and computing power available, it has become pervasive across scientific disciplines. We first highlight why machine learning is needed in marine ecology. Then we provide a quick primer on machine learning techniques and vocabulary. We built a database of ∼1000 publications that implement such techniques to analyse marine ecology data. For various data types (images, optical spectra, acoustics, omics, geolocations, biogeochemical profiles, and satellite imagery), we present a historical perspective on applications that proved influential, can serve as templates for new work, or represent the diversity of approaches. Then, we illustrate how machine learning can be used to better understand ecological systems, by combining various sources of marine data. Through this coverage of the literature, we demonstrate an increase in the proportion of marine ecology studies that use machine learning, the pervasiveness of images as a data source, the dominance of machine learning for classification-type problems, and a shift towards deep learning for all data types. This overview is meant to guide researchers who wish to apply machine learning methods to their marine datasets.},
  file = {C\:\\Users\\cleap\\Zotero\\storage\\NFDB4R3W\\Rubbens et al. - 2023 - Machine learning in marine ecology an overview of.pdf;C\:\\Users\\cleap\\Zotero\\storage\\UF2BBCZI\\7236451.html}
}

@article{schallDeepLearningMarine2024,
  title = {Deep Learning in Marine Bioacoustics: A Benchmark for Baleen Whale Detection},
  shorttitle = {Deep Learning in Marine Bioacoustics},
  author = {Schall, Elena and Kaya, Idil Ilgaz and Debusschere, Elisabeth and Devos, Paul and Parcerisas, Clea},
  date = {2024-04},
  journaltitle = {Remote Sensing in Ecology and Conservation},
  issn = {2056-3485},
  doi = {10.1002/rse2.392},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rse2.392},
  urldate = {2024-05-29},
  abstract = {Passive acoustic monitoring (PAM) is commonly used to obtain year-round continuous data on marine soundscapes harboring valuable information on species distributions or ecosystem dynamics. This continuously increasing amount of data requires highly efficient automated analysis techniques in order to exploit the full potential of the available data. Here, we propose a benchmark, which consists of a public dataset, a well-defined task and evaluation procedure to develop and test automated analysis techniques. This benchmark focuses on the special case of detecting animal vocalizations in a real-world dataset from the marine realm. We believe that such a benchmark is necessary to monitor the progress in the development of new detection algorithms in the field of marine bioacoustics. We ultimately use the proposed benchmark to test three detection approaches, namely ANIMAL-SPOT, Koogu and a simple custom sequential convolutional neural network (CNN), and report performances. We report the performance of the three detection approaches in a blocked cross-validation fashion with 11 site-year blocks for a multi-species detection scenario in a large marine passive acoustic dataset. Performance was measured with three simple metrics (i.e., true classification rate, noise misclassification rate and call misclassification rate) and one combined fitness metric, which allocates more weight to the minimization of false positives created by noise. Overall, ANIMAL-SPOT performed the best with an average fitness metric of 0.6, followed by the custom CNN with an average fitness metric of 0.57 and finally Koogu with an average fitness metric of 0.42. The presented benchmark is an important step to advance in the automatic processing of the continuously growing amount of PAM data that are collected throughout the world's oceans. To ultimately achieve usability of developed algorithms, the focus of future work should be laid on the reduction of the false positives created by noise.},
  langid = {english},
  keywords = {Baleen whales,big data,deep learning,marine bioacoustics,passive acoustic monitoring (PAM),sound detection},
  file = {C\:\\Users\\cleap\\Zotero\\storage\\M9DR75Y7\\Schall et al. - Deep learning in marine bioacoustics a benchmark .pdf;C\:\\Users\\cleap\\Zotero\\storage\\7WPYMUKC\\rse2.html}
}

@article{schallRobustMethodAutomatically2022,
  title = {A {{Robust Method}} to {{Automatically Detect Fin Whale Acoustic Presence}} in {{Large}} and {{Diverse Passive Acoustic Datasets}}},
  author = {Schall, Elena and Parcerisas, Clea},
  date = {2022-12},
  journaltitle = {Journal of Marine Science and Engineering},
  volume = {10},
  number = {12},
  pages = {1831},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2077-1312},
  doi = {10.3390/jmse10121831},
  url = {https://www.mdpi.com/2077-1312/10/12/1831},
  urldate = {2023-02-03},
  abstract = {The growing availability of long-term and large-scale passive acoustic recordings open the possibility of monitoring the vocal activity of elusive oceanic species, such as fin whales (Balaenoptera physalus), in order to acquire knowledge on their distribution, behavior, population structure and abundance. Fin whales produce low-frequency and high-intensity pulses, both as single vocalizations and as song sequences (only males) which can be detected over large distances. Numerous distant fin whales producing these pulses generate a so-called chorus, by spectrally and temporally overlapping single vocalizations. Both fin whale pulses and fin whale chorus provide a distinct source of information on fin whales present at different distances to the recording location. The manual review of vast amounts of passive acoustic data for the presence of single vocalizations and chorus by human experts is, however, time-consuming, often suffers from low reproducibility and in its entirety, it is practically impossible. In this publication, we present and compare robust algorithms for the automatic detection of fin whale choruses and pulses which yield good performance results (i.e., false positive rates {$<$} 3\% and true positive rates {$>$} 76\%) when applied to real-world passive acoustic datasets characterized by vast amounts of data, with only a small proportion of the data containing the target sounds, and diverse soundscapes from the Southern Ocean.},
  issue = {12},
  langid = {english},
  keywords = {<i>Balaenoptera physalus</i>,20 Hz pulse,automatic detection,chorus,fin whale,kurtosis},
  file = {C:\Users\cleap\Zotero\storage\MPIGA7TG\Schall and Parcerisas - 2022 - A Robust Method to Automatically Detect Fin Whale .pdf}
}

@article{schmidlinComparisonEffectsReef2024,
  title = {Comparison of the Effects of Reef and Anthropogenic Soundscapes on Oyster Larvae Settlement},
  author = {Schmidlin, Sarah and Parcerisas, Clea and Hubert, Jeroen and Watson, Maryann S. and Mees, Jan and Botteldooren, Dick and Devos, Paul and Debusschere, Elisabeth and Hablützel, Pascal I.},
  date = {2024-05-31},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {14},
  number = {1},
  pages = {12580},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-63322-2},
  url = {https://doi.org/10.1038/s41598-024-63322-2},
  urldate = {2024-06-01},
  abstract = {Settlement is a critical period in the life cycle of marine invertebrates with a planktonic larval stage. For reef-building invertebrates such as oysters and corals, settlement rates are predictive for long-term reef survival. Increasing evidence suggests that marine invertebrates use information from ocean soundscapes to inform settlement decisions. Sessile marine invertebrates with a planktonic stage are particularly reliant on environmental cues to direct them to ideal habitats. As gregarious settlers, oysters prefer to settle amongst members of the same species. It has been hypothesized that oyster larvae from species Crassostrea virginica and Ostrea angasi use distinct conspecific oyster reef sounds to navigate to ideal habitats. In controlled laboratory experiments we exposed Pacific Oyster Magallana gigas larvae to anthropogenic sounds from conspecific oyster reefs, vessels, combined reef-vessel sounds as well as off-reef and no speaker controls. Our findings show that sounds recorded at conspecific reefs induced higher percentages of settlement by about 1.44 and 1.64 times compared to off-reef and no speaker controls, respectively. In contrast, the settlement~increase compared to the no speaker control was non-significant for vessel sounds (1.21 fold), combined reef-vessel sounds (1.30 fold), and off-reef sounds (1.18 fold). This study serves as a foundational stepping stone for exploring larval sound feature preferences within this species.},
  langid = {english},
  keywords = {Larvae settlement,Noise pollution,Oyster reef ecology,Settlement cue,Soundscapes,Underwater noise},
  file = {C:\Users\cleap\Zotero\storage\PNPYDSS8\Schmidlin et al. - 2024 - Comparison of the effects of reef and anthropogeni.pdf}
}

@dataset{schmidlinSoundPlaybackFiles2024,
  title = {Sound Playback Files for Oyster {{Magallana}} Gigas Settlement Experiment},
  author = {Schmidlin, Sarah and Parcerisas, Clea and Watson, Maryann S. and Hablützel, Pascal I. and Debusschere, Elisabeth},
  date = {2024},
  doi = {Sound playback files for oyster Magallana gigas settlement experiment},
  url = {https://www.vliz.be/en/imis?module=dataset&dasid=8596}
}

@report{spiesecke22OceanAcoustics,
  title = {2.2. {{Ocean Acoustics}}, in: {{Hoppema}}, {{M}}. {{The Expedition PS129}} of the {{Research Vessel POLARSTERN}} to the {{Weddell Sea}} in 2022. {{Berichte}} Zur {{Polar-}} Und {{Meeresforschung}} = {{Reports}} on {{Polar}} and {{Marine Research}}},
  author = {Spiesecke, Stefanie and Parcerisas, Clea and Roca, Irene T. and Boebel, Olaf and Burkhardt, Elke and Thomisch, Karolin and Van Opzeeland, Ilse},
  pages = {18--52},
  institution = {Alfred Wegener Institute},
  url = {https://epic.awi.de/id/eprint/57797/}
}

@online{wallCollaborativeFrameworkComparative,
  title = {A {{Collaborative Framework}} for {{Comparative Analysis}} of {{Diverse Marine Passive Acoustic Monitoring Data}}},
  author = {Wall, Carrie and McKenna, Megan F. and Hatch, Leila T. and Van Parijs, Sofie M. and Bochenek, Rob and Dugan, Peter and Ryan, John and Anderson, Charles D. and Becker, Kyle and Berchok, Catherine and Biddle, Mathew and Boebel, Olaf and Canino, Adrienne and Canonico, Gabrielle and Davis, Genevieve E. and Frasier, Kaitlin E. and Gedamke, Jason and Haver, Samara M. and Khazmutdinova and Rice, Aaron N. and Rowell, Timothy J. and Schumchenia, Emily and Shyka, Thomas and Staaterman, Erica and Thomisch, Karolin},
  pubstate = {prepublished}
}
